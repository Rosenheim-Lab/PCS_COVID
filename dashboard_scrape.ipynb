{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scrape Dashboard Data for COVID Cases \r\n",
    "## in Pinellas County Schools\r\n",
    "\r\n",
    "In this notebook, we develop tools to scrape and analyze the data contained in Pinellas County School's COVID database. The tools include county wide totals, school by school analyses, and data visualization.\r\n",
    "\r\n",
    "## Navigate to the url and click the submit button\r\n",
    "This web page provides some searchability of the COVID results during the 2021-2022 school year in Pinellas County Schools. We do not need the searchability functionality of the webpage; we need the data contained in the database. To get the whole database, only only needs to click on the `Submit` button without any filters applied. Then the web page dynamically displays a table. \r\n",
    "\r\n",
    "The cell below loads packages, sets the correct url, and clicks the `Submit` button. After that, the notebook picks out the table and creates a dataframe. \r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Load packages\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import selenium\r\n",
    "from selenium import webdriver\r\n",
    "from selenium.webdriver.common.by import By\r\n",
    "from selenium.webdriver.support.ui import WebDriverWait as WDW\r\n",
    "from selenium.webdriver.support import expected_conditions as EC\r\n",
    "import time\r\n",
    "import pandas as pd\r\n",
    "import lxml\r\n",
    "\r\n",
    "#Set URL\r\n",
    "URL = 'https://www.pcsb.org/covid19cases'\r\n",
    "URL_2020_2021 = 'https://www.pcsb.org/Page/34025'\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping the table into a df\r\n",
    "\r\n",
    "Here we create a series of functions that can be used to scrape the data and advance through the pages of tables that are present. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "#Define constants and functions to get table and to go to the next page\r\n",
    "\r\n",
    "ID = 'ui-paging-container'\r\n",
    "DEBUG = False\r\n",
    "\r\n",
    "def debug(*args):\r\n",
    "    if DEBUG == True:\r\n",
    "        print(args)\r\n",
    "\r\n",
    "def get_table(driver):\r\n",
    "    #Access table on each page\r\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\r\n",
    "    table = soup.find_all('table')\r\n",
    "\r\n",
    "    #read the table\r\n",
    "    new_df = pd.read_html(str(table))\r\n",
    "    \r\n",
    "    return new_df[0]\r\n",
    "\r\n",
    "def get_page_indices(driver, ID):\r\n",
    "    paging_buttons = driver.find_element(By.ID, ID).text\r\n",
    "    page_text_indices = [page for page in paging_buttons.split('\\n')]\r\n",
    "    page_numbers = [int(page) for page in page_text_indices if (page != '...')]\r\n",
    "\r\n",
    "    return paging_buttons, page_text_indices, page_numbers\r\n",
    "\r\n",
    "\r\n",
    "def initiate_scraping(url, headless):\r\n",
    "    #Set up selenium web interaction -\r\n",
    "    options = webdriver.ChromeOptions()\r\n",
    "    options.add_argument('--ignore-certificate-errors')\r\n",
    "    options.add_argument('--incognito')\r\n",
    "    if headless == True:\r\n",
    "        options.add_argument('--headless')         #Operates webpage without viewing through Chrome\r\n",
    "    driver = webdriver.Chrome(\"C:/webdrivers/chromedriver.exe\")\r\n",
    "\r\n",
    "    #Open webpage with webdriver, un-comment --headless argument above if you don't want to \r\n",
    "    #view the page. \r\n",
    "    driver.get(url)\r\n",
    "\r\n",
    "    # Wait for pages to fully load for specified amount of time before throwing error.\r\n",
    "    driver.implicitly_wait(10)\r\n",
    "\r\n",
    "    #Now that the web page is open and operable, we need to click on the submit\r\n",
    "    #button. Clicking on the search button allows us to get all of the data in a table. \r\n",
    "    driver.find_element_by_xpath('//*[@id=\"minibaseSubmit65979\"]').click()\r\n",
    "\r\n",
    "    return driver\r\n",
    "\r\n",
    "def click_submit_main(driver):\r\n",
    "    driver.find_element_by_xpath('//*[@id=\"minibaseSubmit65979\"]').click()\r\n",
    "\r\n",
    "def determine_total_pages(url, ID, headless = True):\r\n",
    "    '''\r\n",
    "    This function clicks the submit button, clicks on the ellipsis until the ellipsis is not the last\r\n",
    "    in the list of page indices, and returns the total number of pages to be scraped of data tables.\r\n",
    "    '''\r\n",
    "    #Click submit without filters so that all data are displayed in table over multiple pages\r\n",
    "    driver = initiate_scraping(url, headless)\r\n",
    "\r\n",
    "    for x in range(10000):      #This covers up to 10000 pages of data tables on the website.\r\n",
    "        _, page_text_indices, page_numbers = get_page_indices(driver, ID)\r\n",
    "        if page_text_indices[-1] == '...':\r\n",
    "            print('Iteration ', x, '. Clicking \"...\" to obtain pages after ', str(max(page_numbers)), ';\\n searching for the last page.')\r\n",
    "            next_page_xpath = '//*[@id=\"ui-paging-container\"]/ul/li[' + str(max(page_numbers) + 1) + ']/a'\r\n",
    "            print(next_page_xpath)\r\n",
    "            button = driver.find_element_by_xpath(next_page_xpath)\r\n",
    "            driver.execute_script(\"arguments[0].click();\", button)\r\n",
    "        else:\r\n",
    "            total_pages = max(page_numbers)\r\n",
    "            break\r\n",
    "    \r\n",
    "    new_search_xpath = '//*[@id=\"module-content-64809\"]/div/div[2]/ul/li/div/div[1]/span/span/p[1]/a'\r\n",
    "    new_search_button = driver.find_element_by_xpath(new_search_xpath)\r\n",
    "    driver.execute_script(\"arguments[0].click();\", new_search_button)\r\n",
    "\r\n",
    "    return total_pages, page_text_indices, driver\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "#This cell uses some of the functions above, iterating through to the finish and concatenating the data.\r\n",
    "#It takes approximately 3 seconds per table page to run, so expect long run times as the data grow larger.\r\n",
    "\r\n",
    "DEBUG = True\r\n",
    "\r\n",
    "#Determine the number of pages:\r\n",
    "tot, indices, driver = determine_total_pages(URL, ID, headless=True)\r\n",
    "print('Imitated filterless search to determine total pages: complete...\\n')\r\n",
    "\r\n",
    "#Initiate the page log:\r\n",
    "data_df = pd.DataFrame([])\r\n",
    "print('Empty DataFrame created. \\n')\r\n",
    "\r\n",
    "#Initiate filterless search\r\n",
    "click_submit_main(driver)\r\n",
    "print('Restarting filterless search to scrape data...')\r\n",
    "\r\n",
    "#Set up dictionary container for data troubleshooting:\r\n",
    "data_dict = {}\r\n",
    "print('Empty dictionary ready for temporary data sets.\\n')\r\n",
    "\r\n",
    "#Scrape page 1 data\r\n",
    "temp_df = get_table(driver)\r\n",
    "data_df = pd.concat([data_df, temp_df])\r\n",
    "key = 'Page ' + str(1)\r\n",
    "data_dict |= {key:temp_df}\r\n",
    "print('Data scraped from page 1 table...\\n')\r\n",
    "first_loop = True\r\n",
    "\r\n",
    "#Loop through remaining pages, beginning with page 2\r\n",
    "for page in range(2, tot+1):\r\n",
    "    paging_buttons, page_text_indices, page_numbers = get_page_indices(driver, ID)\r\n",
    "    if page <= max(page_numbers):\r\n",
    "        if first_loop is True: \r\n",
    "            debug('Working in the if loop, first group of pages.')\r\n",
    "            #This is the condition for the first time iterating through the table pages\r\n",
    "            next_page_xpath = '//*[@id=\"ui-paging-container\"]/ul/li[' + str(page) + ']/a'\r\n",
    "            debug('Page 1 iteration ' + str(page) + ', xpath = ' + str(next_page_xpath))\r\n",
    "            button = driver.find_element_by_xpath(next_page_xpath)\r\n",
    "            driver.execute_script(\"arguments[0].click();\", button)\r\n",
    "            time.sleep(2)\r\n",
    "        else:\r\n",
    "            debug('Working in the if loop, subsequent groups of pages.')\r\n",
    "            #This is the condition for the susequent times iterating through the table pages\r\n",
    "            next_page_xpath = '//*[@id=\"ui-paging-container\"]/ul/li[' + str(new_start) + ']/a'\r\n",
    "            new_start = new_start + 1\r\n",
    "            debug('Subsequent page iteration ' + str(page) + ', xpath = ' + str(next_page_xpath))\r\n",
    "            button = driver.find_element_by_xpath(next_page_xpath)\r\n",
    "            driver.execute_script(\"arguments[0].click();\", button)\r\n",
    "            time.sleep(2)\r\n",
    "        #Scrape data and concatenate\r\n",
    "        temp_df = get_table(driver)\r\n",
    "        debug('Data scraped from page ' + str(page) + r' table ...\\n')\r\n",
    "        #debug(temp_df.head())\r\n",
    "        data_df = pd.concat([data_df, temp_df])\r\n",
    "        key = 'Page ' + str(page)\r\n",
    "        data_dict |= {key:temp_df}\r\n",
    "\r\n",
    "    elif page > max(page_numbers):\r\n",
    "        debug('Working from the elif loop (clicking the ellipsis).')\r\n",
    "        #In this case, we click the right ellipsis.\r\n",
    "        next_page_xpath = '//*[@id=\"ui-paging-container\"]/ul/li[' + str(page) + ']/a'\r\n",
    "        debug('Page 1 iteration ' + str(page) + ', xpath = ' + str(next_page_xpath))\r\n",
    "        button = driver.find_element_by_xpath(next_page_xpath)\r\n",
    "        driver.execute_script(\"arguments[0].click();\", button)\r\n",
    "        time.sleep(2)\r\n",
    "        #Scrape data and concatenate\r\n",
    "        temp_df = get_table(driver)\r\n",
    "        debug('Data scraped from page ' + str(page) + r' table ...\\n, represented by \"...\"...')\r\n",
    "        #debug(temp_df.head())\r\n",
    "        data_df = pd.concat([data_df, temp_df])\r\n",
    "        key = 'Page ' + str(page)\r\n",
    "        data_dict |= {key:temp_df}\r\n",
    "        #Set first_loop = False\r\n",
    "        first_loop = False\r\n",
    "        debug(page)\r\n",
    "        debug(first_loop)\r\n",
    "        new_start = 4\r\n",
    "        debug('New counter for constructing x_paths: ', new_start)\r\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration  0 . Clicking \"...\" to obtain pages after  10 ;\n",
      " searching for the last page.\n",
      "//*[@id=\"ui-paging-container\"]/ul/li[11]/a\n",
      "Iteration  1 . Clicking \"...\" to obtain pages after  10 ;\n",
      " searching for the last page.\n",
      "//*[@id=\"ui-paging-container\"]/ul/li[11]/a\n",
      "Imitated filterless search to determine total pages: complete...\n",
      "\n",
      "Empty DataFrame created. \n",
      "\n",
      "Restarting filterless search to scrape data...\n",
      "Empty dictionary ready for temporary data sets.\n",
      "\n",
      "Data scraped from page 1 table...\n",
      "\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 2, xpath = //*[@id=\"ui-paging-container\"]/ul/li[2]/a',)\n",
      "('Data scraped from page 2 table ...\\\\n',)\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 3, xpath = //*[@id=\"ui-paging-container\"]/ul/li[3]/a',)\n",
      "('Data scraped from page 3 table ...\\\\n',)\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 4, xpath = //*[@id=\"ui-paging-container\"]/ul/li[4]/a',)\n",
      "('Data scraped from page 4 table ...\\\\n',)\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 5, xpath = //*[@id=\"ui-paging-container\"]/ul/li[5]/a',)\n",
      "('Data scraped from page 5 table ...\\\\n',)\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 6, xpath = //*[@id=\"ui-paging-container\"]/ul/li[6]/a',)\n",
      "('Data scraped from page 6 table ...\\\\n',)\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 7, xpath = //*[@id=\"ui-paging-container\"]/ul/li[7]/a',)\n",
      "('Data scraped from page 7 table ...\\\\n',)\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 8, xpath = //*[@id=\"ui-paging-container\"]/ul/li[8]/a',)\n",
      "('Data scraped from page 8 table ...\\\\n',)\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 9, xpath = //*[@id=\"ui-paging-container\"]/ul/li[9]/a',)\n",
      "('Data scraped from page 9 table ...\\\\n',)\n",
      "('Working in the if loop, first group of pages.',)\n",
      "('Page 1 iteration 10, xpath = //*[@id=\"ui-paging-container\"]/ul/li[10]/a',)\n",
      "('Data scraped from page 10 table ...\\\\n',)\n",
      "('Working from the elif loop (clicking the ellipsis).',)\n",
      "('Page 1 iteration 11, xpath = //*[@id=\"ui-paging-container\"]/ul/li[11]/a',)\n",
      "('Data scraped from page 11 table ...\\\\n, represented by \"...\"...',)\n",
      "(11,)\n",
      "(False,)\n",
      "('New counter for constructing x_paths: ', 4)\n",
      "('Working in the if loop, subsequent groups of pages.',)\n",
      "('Subsequent page iteration 12, xpath = //*[@id=\"ui-paging-container\"]/ul/li[4]/a',)\n",
      "('Data scraped from page 12 table ...\\\\n',)\n",
      "('Working in the if loop, subsequent groups of pages.',)\n",
      "('Subsequent page iteration 13, xpath = //*[@id=\"ui-paging-container\"]/ul/li[5]/a',)\n",
      "('Data scraped from page 13 table ...\\\\n',)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "print(data_df.shape)\r\n",
    "print(data_dict.keys())\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(296, 7)\n",
      "dict_keys(['Page 1', 'Page 2', 'Page 3', 'Page 4', 'Page 5', 'Page 6', 'Page 7', 'Page 8', 'Page 9', 'Page 10', 'Page 11', 'Page 12', 'Page 13'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "pd.DataFrame.to_csv(data_df, 'data_dump_20210816')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "#Slice the dataframe for the data\r\n",
    "data_df['Total cases'] = sum(data_df['Number of positive employees'], data_df['Number of positive students'])\r\n",
    "print(data_df.columns)\r\n",
    "print('Total cases in district = ' + str(sum(data_df['Total cases'])))\r\n",
    "print(data_df.groupby('Locations affected').sum().sort_values(by='Total cases', ascending=False))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index([                             0,                              1,\n",
      "                                    2,                         'Date',\n",
      "                 'Locations affected', 'Number of positive employees',\n",
      "        'Number of positive students',                  'Total cases'],\n",
      "      dtype='object')\n",
      "Total cases in district = nan\n",
      "                                           Number of positive employees  \\\n",
      "Locations affected                                                        \n",
      "Administration Building                                            17.0   \n",
      "Palm Harbor University High School                                  1.0   \n",
      "Ponce de Leon Elementary School                                     0.0   \n",
      "Plumb Elementary School                                             1.0   \n",
      "Pinellas Technical College St. Petersburg                           4.0   \n",
      "...                                                                 ...   \n",
      "Garrison-Jones Elementary School                                    5.0   \n",
      "Fuguitt Elementary School                                           1.0   \n",
      "Frontier Elementary School                                          2.0   \n",
      "Forest Lakes Elementary School                                      1.0   \n",
      "Woodlawn Elementary School                                          0.0   \n",
      "\n",
      "                                           Number of positive students  \\\n",
      "Locations affected                                                       \n",
      "Administration Building                                            0.0   \n",
      "Palm Harbor University High School                                12.0   \n",
      "Ponce de Leon Elementary School                                    1.0   \n",
      "Plumb Elementary School                                            1.0   \n",
      "Pinellas Technical College St. Petersburg                          0.0   \n",
      "...                                                                ...   \n",
      "Garrison-Jones Elementary School                                   0.0   \n",
      "Fuguitt Elementary School                                          2.0   \n",
      "Frontier Elementary School                                         3.0   \n",
      "Forest Lakes Elementary School                                     2.0   \n",
      "Woodlawn Elementary School                                         2.0   \n",
      "\n",
      "                                           Total cases  \n",
      "Locations affected                                      \n",
      "Administration Building                            0.0  \n",
      "Palm Harbor University High School                 0.0  \n",
      "Ponce de Leon Elementary School                    0.0  \n",
      "Plumb Elementary School                            0.0  \n",
      "Pinellas Technical College St. Petersburg          0.0  \n",
      "...                                                ...  \n",
      "Garrison-Jones Elementary School                   0.0  \n",
      "Fuguitt Elementary School                          0.0  \n",
      "Frontier Elementary School                         0.0  \n",
      "Forest Lakes Elementary School                     0.0  \n",
      "Woodlawn Elementary School                         0.0  \n",
      "\n",
      "[125 rows x 3 columns]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "5a06ee7dfc3be38b346c7eeec74a298f806dd2de984ba6d6514b00d6ac1942b5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}